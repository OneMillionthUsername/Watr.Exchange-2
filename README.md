# Watr.Exchange Solution Architecture Overview

## Project Structure and Components

The Watr.Exchange solution is organized into multiple .NET projects, each encapsulating a layer or concern of the system. Below is a breakdown of the active projects (excluding the inactive **Business.Proxy** and **Web.REST** components):

* **Watr.Exchange.Core:** Defines core domain interfaces and types used across the system. This includes fundamental interfaces (e.g. `IActor`, `IObject`) and enumerations describing domain concepts (for example, user/actor roles like **Admin**, **Expert**, **Patron**, etc.). These core definitions establish the domain model contracts (e.g. an `IActor` has a Name and Email) and enumerated categories such as `ActorTypes` (Admin, Expert, Patron) and sub-types (e.g. Investor types) that are used by other layers.

* **Watr.Exchange.Data.Core:** Implements the graph-based domain data model. It provides abstract base classes for **graph entities** (vertices and edges) and defines concrete domain vertex types. All domain objects are represented as graph vertices derived from a common `GraphObject` base (with standard fields like `Id`, timestamps, etc.). For example, an abstract `Actor` vertex class extends the base `Vertex` and adds actor-specific properties (Name, Type, Stereotype). This project also defines interfaces for graph relationships (edges) and base edge classes, laying the groundwork for linking vertices (e.g. an edge could represent an **association between an investor and an asset** in the graph).

* **Watr.Exchange.Data.Commands.Core:** Defines the **write operations** (commands) in a CQRS style. It declares generic command request types using MediatR (e.g. `CreateVertex<T>` to add a new vertex, `UpdateVertex<T>` to modify one). These are simple DTOs representing intentions to mutate the graph (create, update, soft-delete, or hard-delete a vertex). For instance, `CreateVertex<TVertex>` is a MediatR request carrying a vertex object to be added to the graph and expecting the new entity’s ID as a result.

* **Watr.Exchange.Data.Queries.Core:** Defines the **read operations** (queries) in the CQRS pattern. It provides generic query request types (using MediatR’s IRequest/IStreamRequest) for fetching data from the graph. Examples include `GetVertexById<T>` to retrieve a vertex by its Id and `GetVertexes<T>` for querying multiple vertices with optional filtering, ordering, and paging. These query types encapsulate read parameters (like filter expressions or page size) but not the actual data access logic.

* **Watr.Exchange.Data:** Implements the **graph database integration** and coordinates command/query handlers. This project ties the core data models and command/query definitions to an actual graph database (e.g. Azure Cosmos DB Gremlin API or another TinkerPop Gremlin store). It likely configures an `IGremlinQuerySource` (the Gremlinq session/connection to the DB) and registers MediatR handlers that execute commands/queries against the graph. For example, the handler for create commands uses Gremlinq to add a new vertex: it calls the Gremlin query source to `AddV` (add vertex) and returns the generated Id. Similarly, query handlers use Gremlinq to traverse the graph – for instance, fetching a vertex by Id via `G.V(request.Id)…FirstOrDefaultAsync`. The Data project organizes these handlers and ensures they are injectable services. (Note: The actual configuration details are abstracted, but the presence of Gremlin queries in the handlers indicates a graph DB is in use.)

* **Watr.Exchange.Data.Commands:** Contains the **MediatR command handlers** for data modification. It provides concrete implementations for the command requests defined in Data.Commands.Core. These handlers encapsulate how the system creates, updates, or deletes vertices in the graph. For example, the `CreateCommandHandler` for a vertex type uses the Gremlinq `IGremlinQuerySource` to add the vertex to the graph and returns its Id. There are also handlers for updates (which find the existing vertex, merge changes, and update it) and deletions (which may mark an `IsDeleted` flag or physically drop the vertex). This separation ensures that all writes to the database go through a well-defined pipeline.

* **Watr.Exchange.Data.Queries:** Contains the **MediatR query handlers** for data retrieval. This includes handlers for the queries defined in Data.Queries.Core. They use Gremlinq to read from the graph database. For instance, a `GetVertexByIdHandler` will query the graph for a vertex with the given Id and return it (or null). Another handler, `GetVertexesHandler`, executes a graph traversal with optional filters and ordering, streaming results as an async enumerable. These handlers make it easy for higher layers to request data without dealing directly with Gremlin syntax – they just send query requests via MediatR and receive domain objects or DTOs.

* **Watr.Exchange.Business.Core:** Defines the **business layer contracts and DTOs**. It contains Data Transfer Objects and interfaces that represent the business operations. For example, it defines *CreateDTO*/*ReadDTO*/*UpdateDTO* shapes for each domain entity, which mirror the data model but are used for input/output at the business layer. In this project, you find definitions like `CreateActorDTO` (implementing `IActor` and carrying fields to create an Actor) and numerous subclasses for specific actor types (CreateAdminDTO, CreateInvestorDTO, etc.). It also likely declares interfaces for business operations – e.g. `ICreateActivity` or `IProcessor` – that the concrete business logic will implement. Essentially, Business.Core is the contract and model definition layer for the business logic.

* **Watr.Exchange.Business:** Implements the **business logic and orchestration** using the patterns defined in Business.Core. This is a key project that coordinates between the UI/service layer and the data layer. The Business project uses a combination of *Activities* and *Processors* to represent use cases. An **Activity** in this context is a unit of business logic for a specific operation on a specific entity (for example, “Create Admin User” or “Update Investor Profile”). The Business classes derive from generic base classes to implement concrete behaviors. For instance, a `CreateActivity<...>` subclass will handle creating a particular entity type. The pattern (leveraging MediatR and AutoMapper) is as follows for a create operation: the Activity’s `Create(...)` method maps the input DTO into a graph vertex object and sends a create-command via MediatR to the data layer, then on success it queries back the newly created object. This sequence is transaction-like: it ensures that after creation, the latest data (with generated IDs or timestamps) is retrieved before returning. Business activities thus **encapsulate multi-step workflows** (create -> persist -> read) but expose them as simple methods to the outside. In addition to Activities, the Business layer defines **Processors**, which group related activities (CRUD) for a domain object. A `Processor` might bundle create, read, update, delete operations for an entity type into one higher-level service object. For example, a `CRUDProcessorProxy` will hold references to the corresponding CreateActivity, ReadActivity, etc., and expose unified methods (like `.Create`, `.Update`) that internally delegate to the proper Activity. This provides a convenient façade for upper layers to perform full lifecycle operations. Overall, the Business project is the domain logic hub: it enforces any business rules, orchestrates calls to Data (via MediatR), and uses mapping to translate between internal models and external-facing DTOs.

* **Watr.Exchange.Mapping.Core:** Contains the **object mappings** (AutoMapper profiles) that translate between the graph data models and the business DTOs. Because the system separates the data entities (graph vertices) from the DTOs used in business logic, this project defines how to convert between them. For example, it includes an `ActorMappingProfile` that maps create/update Actor DTOs to the corresponding `Actor` vertex subclasses. It specifies rules such as generating a new GUID Id and setting timestamps when mapping from a CreateDTO to a new Vertex. Similarly, it maps updated DTOs onto existing vertex objects (taking care to ignore or handle metadata like `ETag`, `IsDeleted`). The Mapping profiles are used by the Business layer (via AutoMapper) to ensure that when an Activity calls the data layer, it passes properly translated domain objects. In short, this project isolates all translation logic so that other parts of the system can work with high-level DTOs without worrying about graph-specific details.

* **Watr.Exchange.Security.Core:** Defines the **authentication/authorization contracts** for the platform. It declares interfaces and models for the security functionality – for example, an `ISecurityProvider` interface that abstracts user identity and token acquisition. This interface includes properties/methods such as checking if a user is authenticated and methods to log in or log out and to retrieve access tokens. By abstracting these, the UI or business logic can remain identity-provider-agnostic.

* **Watr.Exchange.Security:** Implements core security logic (likely for token management, user info, etc.) not tied to a UI. This project might contain implementations of the security provider interface, integration with external identity services, and any domain models for users/roles if needed. It works in tandem with Security.Core to provide concrete behavior (for instance, using OAuth2/OpenID Connect to authenticate). Details in the repository indicate integration with Azure AD B2C (via an MSAL library) for handling user authentication.

* **Watr.Exchange.Security.Blazor:** A specialization of the security implementation for Blazor/WebAssembly. This provides an `ISecurityProvider` implementation that hooks into Blazor’s authentication mechanisms. For example, the `SecurityProvider` class here uses the Blazor WebAssembly `IAccessTokenProvider` and `AuthenticationStateProvider` to manage login state and tokens. It implements the login flow by attempting silent token acquisition and triggering interactive browser redirects if needed (through the Blazor `NavigationManager`). Essentially, this project makes it seamless for the Blazor frontend to authenticate via Azure AD B2C and obtain JWT access tokens for calling protected APIs. It’s registered as the concrete `ISecurityProvider` in the Blazor app, so the UI can simply call methods like `Login()` or `GetAccessToken()` and this provider handles the underlying MSAL logic and state change notifications.

* **Watr.Exchange.ViewModels:** Contains the **presentation layer view-models** used by the client applications (Web and MAUI). These view-model classes (implemented with ReactiveUI and MVVM patterns) manage UI state and interactions, decoupling the views from business logic. For example, there is a `MainViewModel` that tracks whether a user is logged in and the username, exposing reactive commands to log in or out. It listens for authentication state changes via the `ISecurityProvider` and updates the UI state accordingly. The use of ReactiveUI means that these view-models support reactive data binding and asynchronous commands, which is well-suited for rich client apps. This project ensures a consistent approach to state management between the Blazor UI and the MAUI app – both can utilize the same view-models for things like authentication, navigation, and user notifications (e.g. an `AlertViewModel` for showing alerts).

* **Watr.Exchange.Web:** The **Blazor WebAssembly client application** providing the web UI. This project is essentially a front-end delivered to browsers. It is built with Blazor WebAssembly and uses the MudBlazor component library for the UI look-and-feel. In this project, the Program.cs configures the app to use required services: for instance, it adds MudBlazor services, sets up a scoped `HttpClient` for API calls, and configures MSAL authentication for Azure AD B2C integration. The MSAL setup points to the Azure AD B2C authority and client ID, enabling the Blazor app to handle sign-in/sign-out and token retrieval seamlessly. The Blazor app likely consumes the `Watr.Exchange.ViewModels` and `Watr.Exchange.Security.Blazor` components – for example, it injects the `ISecurityProvider` so that UI pages can call `Login` or show `Logout` links. The routing and pages (e.g., Razor components for main page, login display, etc.) live here, but they rely on the shared view-models for logic. Essentially, **Watr.Exchange.Web is the single-page application** that end users interact with, providing a reactive UI and calling back-end services (the REST API, when active) via HttpClient. It does not itself contain business logic or data access; it delegates those concerns to the backend or to the view-model layer.

* **Watr.Exchange.Client.MAUI:** The **.NET MAUI cross-platform client** application. This project targets mobile and desktop platforms (Android, iOS, Windows, Mac) and provides an alternative client to the Blazor web UI. It shares much of the presentation logic with the Blazor app via the ViewModels project. The MAUI app likely uses XAML pages that bind to these view-models, and it references the core projects to perform operations. In the project file, we see references to Business.Core, Security, ViewModels, etc., indicating the MAUI app can call directly into the business layer or through an abstraction. In practice, the MAUI app would authenticate users (possibly using a native MSAL library or a WebView for B2C) and then interact with the Exchange functionality. It might use the **Business.Proxy** as a client-side facade to call the server (if it were fully implemented), but since Business.Proxy is excluded/likely not fully implemented, the MAUI app might currently be limited to demoing local functionality or would need to call the REST API endpoints directly. In summary, Watr.Exchange.Client.MAUI is architected to be a first-class client, reusing the same business logic and models as the web, thus adhering to DRY principles across platforms.

*(Additionally, the solution contains a test project **Watr.Exchange.Business.Mappings.Test** focusing on verifying the AutoMapper configurations. This indicates the team’s practice of writing unit tests for critical mapping logic – ensuring that every DTO ↔ Entity conversion is correct and complete.)*

## Runtime Operation Sequencing (Order & Asset Lifecycles)

One of the goals of the Watr.Exchange architecture is to clearly delineate how runtime operations flow through the system’s layers. Major business processes – such as the lifecycle of an order or an asset – are handled in a sequenced manner involving the UI, business Activities, and data commands/queries. Below, we outline the sequence of steps for two key examples (order processing and asset management) and highlight the interactions between components:

### **Order Lifecycle** (e.g. an investor placing an order to purchase an asset):

1. **Initiation from UI:** The process begins at the client (MAUI app or Blazor web). Suppose an *Investor* user (a Patron) fills out an order form and submits a new order. The UI layer (view or page) invokes the appropriate operation on the business layer. In the Blazor app, for example, this could be done via a view-model command bound to a form submission. The view-model would call into a **Business Processor** or Activity responsible for orders (e.g. something like `OrderProcessor.CreateOrder.Create(...)`). This call enters the Business logic layer with a *CreateOrder DTO* containing the order details.

2. **Business Layer – Create Activity:** The Business **CreateActivity** for the Order handles the request. Internally, it uses AutoMapper to convert the incoming DTO into the corresponding data model object (a new Order vertex). The Activity then uses MediatR to send a `CreateVertex<Order>` command to the Data layer. At this point, control moves to the asynchronous pipeline handled by MediatR. (The Business layer does not directly talk to the database; it formulates a command and relies on the Data layer’s handlers.)

3. **Data Layer – Command Handling:** The MediatR infrastructure routes the command to the **Data.Commands** handler for `CreateVertex<Order>`. This handler, using the configured Gremlin query source, **persists the new Order** in the graph database. It does so by adding a vertex of the appropriate type with all the mapped properties. The handler likely also sets metadata (timestamps, etc., if not already set by the mapper). Upon successful creation, the handler returns the newly assigned Id (as a string/Guid) for the Order vertex back to the Business layer.

4. **Business Layer – Read Back Result:** The Business CreateActivity, having received the new Order’s Id, now performs a follow-up query to get the full Order data that was saved. It does this by sending a `GetVertexById<Order>` query via MediatR (leveraging a paired **ReadActivity** or simply invoking the query through Mediator). The Data.Queries handler for this request executes a Gremlin traversal to fetch the Order vertex by its Id. The vertex is retrieved from the graph (including any default fields like created timestamp, etc.) and returned to Business.

5. **Mapping & Return to UI:** The Business layer maps the retrieved Order vertex back into a **ReadDTO** (for example, an Order DTO that includes the newly assigned Id and any system-generated fields). This DTO is then returned to the caller. In a Blazor app, this might update the UI state (e.g. navigate to an order confirmation page or display the newly created order in a list). In a MAUI app, the view-model would now hold the created order data for the UI to bind. The **order creation** round-trip is complete: the UI initiated it, business logic validated and processed it, and the data layer committed it to the graph database, with the result flowing back to the UI.

6. **Further Order Lifecycle Steps:** After creation, an order in an exchange typically goes through additional states: e.g. *open/pending*, *matched/fulfilled*, or *canceled*. In this architecture, those would be handled by **Update and Delete Activities** in the Business layer. For instance, if an order is canceled, the UI would trigger a delete operation which calls a Business `DeleteActivity` for orders. That activity would send a `DeleteVertex` command to the data layer, whose handler might mark the Order’s `IsDeleted` flag to true (a soft delete). If an order is fulfilled (matched to a sell order, for example), the business might invoke an update that changes an “OrderStatus” property from “Open” to “Filled” and perhaps creates a related **Transaction** record. That update would flow through a similar pipeline: the Business `UpdateActivity` maps a DTO to the Order vertex, sends an `UpdateVertex<Order>` command, and the Data handler writes the changes to the graph. In all cases, the **sequence of steps remains consistent** – the business layer coordinates the operation and uses the data layer via commands/queries, ensuring that the core business rules are applied in one place and the database is accessed only through well-defined handlers.

7. **Relationships in Order Processing:** It’s worth noting that an *Order* in the context of an exchange involves relationships to other entities: the investor who placed it (a Patron, which is an Actor) and the asset or listing being purchased. In a graph database design, these relationships can be naturally represented as edges. For example, when an Order is created, the system might also create an edge like `PlacedBy` from the Order vertex to the Actor (Investor) vertex, and an edge like `ForAsset` from the Order to the Asset vertex. These edges would be established by additional commands in the data layer (perhaps via specialized CreateEdge commands). While the current implementation code focuses on vertex operations, the **foundation for edges exists** in Data.Core. This means the system is designed to support capturing the full graph of an order’s context, enabling queries such as “find all orders placed by a given investor” or “find all orders for a particular asset” via graph traversals. The lifecycle of an order, therefore, is not only about the vertex representing that order but also about maintaining these links as the order moves through stages (e.g. linking to a transaction or trade execution edge once fulfilled).

### **Asset Lifecycle** (e.g. creating and managing an asset/listing on the exchange):

1. **Asset Creation:** Assets (or listings of an investment opportunity, commodity, etc.) are typically created by an admin or authorized user (perhaps an *Expert or Admin* actor in the system). The creation of an asset would be handled similarly to orders: the user initiates the action on the UI (filling out asset details), and the UI calls a Business `CreateActivity` for assets with a *CreateAssetDTO*. The Business layer maps this to a new Asset vertex and sends a `CreateVertex<Asset>` command to the data handlers. The Data layer adds the Asset vertex to the graph database and returns its new Id. The Business logic then queries back the created Asset (now with its Id and default fields) and returns an Asset DTO to the UI. At this point, the asset is registered in the system as an available item. (The **technology stack** and pattern used here is the same as with orders – MediatR commands and Gremlin queries – so the creation sequence is analogous to the order creation flow described above.)

2. **Asset-State Transitions:** Once an asset is created (e.g. a new listing is live), it may go through a lifecycle of states: for example, *Open* (accepting orders), *Closed* (no longer available), *Funded* or *Settled* (if it represents a fundraising target reached or a trade completed). These changes would be performed via **Update operations** in the Business layer. For instance, closing an asset to new orders might involve setting an “IsOpen” flag to false or updating a status field – the Business `UpdateActivity` for Asset would create an `UpdateVertex<Asset>` command carrying the new state, and the Data handler would apply that to the stored vertex. The updated state would be read back and delivered to the UI to inform users that the asset is closed. Similarly, if there’s a concept of partial funding or progress, the asset’s properties (like percentage funded) could be updated periodically through the same mechanism.

3. **Order-Asset Interaction:** An asset’s lifecycle is tightly coupled with orders. When investors place orders against an asset, the system needs to reflect that relationship. As mentioned, the graph model would allow edges from Asset to Order (e.g., an edge per order indicating an investor’s commitment). In practice, when an order is created for an asset, the Business logic could also handle creating a linking edge, or the data layer could automatically create edges as part of order creation if the models are set up to do so. For example, if an Order vertex has a reference to Asset Id, the CreateOrder handler might attach the order to the asset in the graph. Over the asset’s life, one might query how many orders or what total amount has been invested – queries that Gremlin can answer by traversing edges from the Asset vertex to related Order vertices.

4. **Asset Closure and Deletion:** At the end of an asset’s lifecycle (for instance, a project funding period ends or an asset is withdrawn), an admin may close it. This could be an update marking it as closed or even a deletion operation. A *soft delete* via a `DeleteVertex` command might mark the asset’s `IsDeleted` flag, which the system would interpret as “no longer available.” Because the system is graph-based, even a deleted asset vertex can remain connected to historical orders, preserving data lineage (since soft-deleted vertices can still be queried if needed). A *hard delete*, on the other hand, would remove the asset vertex entirely and, by cascade, its connected edges (the Data layer’s `HardDeleteVertex` handler uses Gremlin’s `.Drop()` to remove the vertex). Hard deletes would typically be used only if truly erasing an asset is required, as they permanently sever links in the graph.

5. **Sequential Logic and Notifications:** Throughout the asset lifecycle, the **sequence of operations is carefully managed by the Business layer**. Because the Business Activities wrap the data calls, they can include additional logic like validation or triggering notifications. For example, before creating an asset, the Business layer could enforce that the user has an “Admin” role. After closing an asset, it could trigger an Activity to notify all investors (perhaps by creating “Notification” vertices or sending emails via an external service). The architecture’s decoupling means such enhancements affect only the Business layer code; the UI still just invokes a high-level operation (e.g. CloseAsset), and the Data layer simply executes the commands it is given. The **ordering of operations** (for example, create asset -> then allow orders -> then close asset -> possibly payout funds) can be orchestrated by higher-level processors or workflows in the Business layer, potentially using the **Processor** constructs. The `CRUDProcessorProxy` pattern allows grouping these actions so that a single object in Business can manage the whole lifecycle of an asset or order with a unified interface. This makes it easier to sequence actions in code (e.g., using an AssetProcessor to first call `.Create`, later call `.Update` or `.Delete` on the same asset entity).

In summary, **the runtime flow** for any operation in Watr.Exchange follows a consistent pattern: the request originates from the presentation layer (MAUI or Blazor), goes into a Business layer **Activity/Processor** which handles all business rules and uses **MediatR** to delegate persistence to the Data layer, where **graph database operations** occur, and results bubble back up following the reverse path. This design ensures a clear separation of concerns: UI concerns (user input and display) are separated from business logic (which knows the sequence and rules of operations), and those in turn are separated from data access (which knows how to query/update the graph). The sequence is linear and traceable for each use case (order, asset, user management, etc.), making the system easier to reason about and extend. For example, to introduce a new operation (say, transferring ownership of an asset), one would implement a new Business Activity (ensuring the proper sequence of commands: perhaps create a Transfer record, update asset owner, etc.) and corresponding Data handlers, without needing to tangle UI code with database calls or vice versa.

## Technology Stack and Design Patterns

The Watr.Exchange solution is built on a modern .NET stack with a strong emphasis on **clean architecture principles** (separating core domain, business logic, and infrastructure) and uses a variety of frameworks and libraries to achieve its goals:

* **Microsoft .NET 7/8 (C#)**: The core language and runtime for all projects. The solution is organized as a .NET solution with class library projects for each layer and target-specific projects for the clients. The use of .NET MAUI and Blazor WebAssembly places the solution at the forefront of cross-platform .NET development, targeting Android, iOS, Windows, macOS (via MAUI) and all web browsers (via WebAssembly).

* **Graph Database (Gremlin API)**: The back-end database is a **graph database**, which is evident from the use of the Gremlin query language constructs in the data layer. The code uses the ExRam **Gremlinq** library (a LINQ provider for Gremlin) to interact with the graph database. This likely corresponds to using **Azure Cosmos DB (Gremlin API)** or another TinkerPop-compatible graph database (such as Apache TinkerPop/JanusGraph or AWS Neptune) to store all domain data as vertices and edges. The choice of a graph DB suits the domain, as relationships between actors, assets, and orders are first-class and can be traversed efficiently. Gremlinq allows writing Gremlin queries in C# LINQ syntax, which the data layer does (e.g., `G.V<Actor>()...` for queries or `G.AddV(vertex)` for inserts).

* **CQRS and MediatR**: The architecture follows a **CQRS (Command Query Responsibility Segregation)** pattern, separating write and read models and using **MediatR** as the mediator for commands and queries. MediatR is a popular .NET library for in-process messaging – here it is used to dispatch command and query objects to their handlers. This decouples the business logic from the data access implementation. For example, a Business Activity doesn’t call the database directly; it calls `Mediator.Send(new CreateVertex<Order>(order))` and MediatR routes that to the appropriate handler in Data.Commands. The configuration of MediatR in the project means all command/query handlers are registered and the mediator is injected where needed. This pattern provides **clear separation and testing advantages**, as each handler can be tested in isolation and business operations can be unit-tested with a mock MediatR that returns fake data. The code explicitly uses `IRequest<T>` and `IRequestHandler<T>` interfaces from MediatR for commands/queries, confirming the CQRS design.

* **AutoMapper**: To bridge the gap between the object models (DTO vs. Domain Vertex), Watr.Exchange uses **AutoMapper** for object-object mapping. The Mapping.Core project contains Profile classes where mappings are defined in detail. AutoMapper reduces manual mapping code and ensures consistency. For instance, when converting a `CreateInvestorDTO` to an `Investor` vertex, AutoMapper will apply all the `.ForMember` rules (like generating a new Id, timestamps, etc.) as defined in the profile. This yields cleaner business logic code – the Activities simply call `Mapper.Map<TVertex>(dto)` and trust that the mapping configuration will produce a fully-formed vertex object ready for persistence. Conversely, mapping is used to project database results back into read DTOs suitable for UI. The extensive mapping configuration in **ActorProfile** (and presumably similar profiles for other entities) reflects the system’s flexibility in handling multiple sub-types and the generic approach to updating different actor types via a single generic DTO using custom type converters. AutoMapper is a crucial part of the tech stack to support this polymorphic behavior.

* **ReactiveUI and MVVM**: On the client side, the solution employs an MVVM (Model-View-ViewModel) design, especially in the MAUI app and even in Blazor to some extent. The **ReactiveUI** framework is used for building reactive view-models that can be shared across UI platforms. ReactiveUI allows for properties that notify on change and command binding to asynchronous operations. We see in the MainViewModel, for example, `ReactiveCommand` is used to wrap the login and logout operations asynchronously. This means the UI can bind buttons to these commands and get automatic loading indicators, error handling, etc., based on the ReactiveUI infrastructure. The use of `.WhenPropertyChanged` in view-models subscribes to changes in the security provider to reactively update the UI state (for login status). The stack includes **XAML** for MAUI pages and **Blazor’s Razor** for web pages, but both leverage the same C# view-models. This choice of ReactiveUI and MVVM indicates a commitment to a **testable and platform-agnostic presentation layer** – the UI rendering is separate from the UI logic, which is all coded in these view-model classes.

* **Blazor WebAssembly & .NET MAUI**: The inclusion of these two front-end technologies showcases the flexibility of the stack:

  * *Blazor WebAssembly* (**Watr.Exchange.Web** project) allows running C# in the browser. The tech stack here includes **WebAssembly** for client-side execution, **HTML/CSS** for layout (with styles and possibly MudBlazor components for a modern look), and **ASP.NET Core components** for hosting the Blazor app (perhaps via a static site or integrated with an ASP.NET backend). The Program configuration reveals integration with **MudBlazor UI components** and **MSAL authentication** for Azure AD B2C, meaning the web app has a polished UI framework and enterprise-grade auth. The Blazor app is entirely client-side – it communicates with the server via HTTP calls. In development, it’s likely calling the Web.REST API (if it were running) at the same origin (note the HttpClient BaseAddress configuration). This makes the web stack purely API-driven and serverless (except for the API endpoints).
  * *.NET MAUI* (**Watr.Exchange.Client.MAUI** project) is used for a native app experience. The tech includes **XAML** UI definitions and platform-specific integrations. The MAUI app references `.NET MAUI Essentials` and UI toolkit packages (e.g., Syncfusion for UI controls, CommunityToolkit, etc.), and importantly it references the shared Watr.Exchange libraries to reuse all non-UI code. The MAUI project file shows multi-targeting for Android, iOS, MacCatalyst, and Windows (indicating .NET 7+ capabilities for cross-platform deployment). It also references **ReactiveUI.Maui**, confirming ReactiveUI is used in the MAUI context as well. With MAUI, the app can have offline capabilities or enhanced device integration if needed, but it likely still requires network connectivity to perform core exchange functions (since data lives in the cloud graph DB).

* **Authentication & Authorization**: The solution uses **Azure AD B2C** (an identity-as-a-service for consumer-facing apps) to handle user authentication. This is evident from the Blazor configuration where MSAL (Microsoft Authentication Library) is set up with an Authority and ClientId corresponding to an Azure B2C tenant. The **MSAL Authentication** integration means the app delegates sign-in to Azure (with support for social identities or custom policies if configured) and obtains JWT access tokens for API calls. The Security.Blazor provider wraps this MSAL usage for the Blazor app, and presumably the MAUI app uses a similar mechanism (MAUI might use MSAL through Xamarin/MAUI libraries to sign in and cache tokens). Token acquisition and renewal flows are abstracted behind the `ISecurityProvider` interface. Once authenticated, **authorization** to specific features (like who can create assets or view certain data) would be enforced in the Business layer or via token scopes/claims. The Security.Core interface hints at usage of scopes when calling `GetAccessToken(string[] scopes)`, meaning the application requests tokens with specific scopes (likely to access a protected Web API). The tech stack here is **OAuth2/OpenID Connect with JWT** for calls between client and server.

* **ASP.NET Core (Web API)**: Although the **Watr.Exchange.Web.REST** project is excluded from this discussion, it’s implied by the overall architecture that an ASP.NET Core Web API would expose the business functionality to clients in a RESTful manner. The Blazor app’s HttpClient and the AD B2C integration suggest that a token-protected web API is expected. In such a stack, ASP.NET Core would host controllers or minimal APIs that authenticate the JWT from the client, then call into the Business layer (probably via the same Processors/Activities, just invoked on the server side). The fact that Business.Proxy exists indicates an intent to possibly allow direct communication or to simplify calling patterns, but with a standard approach, the MAUI and Blazor clients would use **HTTP/JSON** to talk to the server. The tech stack for the server (if implemented) would then include **ASP.NET Core 7**, with potentially **Swagger/OpenAPI** for documentation and **Dependency Injection** configured to register all the MediatR handlers, AutoMapper profiles, etc., on startup. Logging (via `Microsoft.Extensions.Logging`) is used throughout the code, so one can infer integration with ASP.NET Core’s logging providers for diagnostics.

* **Dependency Injection and Modular Design**: Each project is structured to be modular and injectable. For example, the Business layer classes expect interfaces like `IMediator` (from MediatR) and `IMapper` (from AutoMapper) via constructor injection, and these would be provided by the DI container at runtime. The **Microsoft.Extensions.DependencyInjection** framework is the backbone of dependency injection in .NET and is used implicitly (e.g., in Program.cs for Blazor, services are added for the security provider, view-models, etc.). The architecture uses DI to swap implementations (for instance, in tests, one could provide an in-memory Gremlin provider or stubbed MediatR). The layering into separate projects also enforces that, for example, the Business layer cannot directly access Data implementation classes – it must go through interfaces/mediator, which is a deliberate design for maintainability.

* **UI/UX Libraries**: On the front-end, **MudBlazor** (a UI component library based on Material Design for Blazor) is used, which accelerates UI development with ready-to-use components and consistent styling. In MAUI, **Syncfusion Maui Toolkit** is referenced, likely to provide rich UI controls in the native app (charts, grids, etc., as an example). These choices indicate the tech stack values a modern, professional UI for both web and mobile.

* **Testing and Tooling**: The inclusion of a test project and likely use of xUnit or NUnit (common in .NET, though not explicitly seen in snippet) suggests a focus on automated testing. The architecture’s separation of concerns makes it easier to write tests for mapping configurations (as done in Mapping.Test), business logic (by mocking the MediatR and verifying that correct commands are sent for a given input), and even data layer (by running commands against a local test graph or an in-memory substitute). The stack also leverages analyzers and code generators indirectly (for example, JSON serialization attributes are used on enums to ensure they serialize as strings, indicating use of System.Text.Json which is part of .NET Core).

In conclusion, the Watr.Exchange solution is a **comprehensive, enterprise-grade stack**: it uses C# and .NET for cross-platform reach, a graph database for complex relational data, CQRS with MediatR for clear command-query separation, AutoMapper for model transformations, and modern UI frameworks (Blazor and MAUI) for rich client experiences. The architecture emphasizes clear sequencing of operations – each major action flows through well-defined layers – and the technology choices reinforce this by providing structured pipelines (MediatR pipeline, AutoMapper profiles, etc.). This design not only makes the system easier to maintain and extend but also positions it to handle complex domain logic (like trading lifecycles and user role management) with clarity and flexibility. Each project plays a specific role, and together they form an architecture that is **modular, scalable, and aligned with best practices** for a multi-platform, cloud-connected application.
